import requests
import os
import sys
import time
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# ä¸ºæœ¬åœ°å¼€å‘åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
# åœ¨ GitHub Actions ä¸­ï¼Œè¿™å°†æ— å®³åœ°å¤±è´¥æˆ–åŠ è½½ç©ºå†…å®¹ï¼Œä»è€Œä¼˜å…ˆä½¿ç”¨ Actions è‡ªèº«è®¾ç½®çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ä»ç¯å¢ƒå˜é‡ä¸­è¯»å– (å¯èƒ½æ˜¯ Actions, .env, æˆ–ç›´æ¥è®¾ç½®)
TOKEN = os.environ.get("GITHUB_TOKEN")
USERNAME = os.environ.get("GITHUB_REPOSITORY_OWNER")

if not TOKEN or not USERNAME:
    print("é”™è¯¯: GITHUB_TOKEN å’Œ GITHUB_REPOSITORY_OWNER ç¯å¢ƒå˜é‡æœªè®¾ç½®ã€‚")
    print("è¯·ç¡®ä¿åœ¨ GitHub Actions ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬ï¼Œå¹¶å·²è®¾ç½®æ­£ç¡®çš„ç¯å¢ƒå˜é‡ã€‚")
    sys.exit(1)

def get_graphql_data(query, variables):
    """é€šç”¨GraphQLè¯·æ±‚å‡½æ•°"""
    headers = {"Authorization": f"bearer {TOKEN}"}
    for attempt in range(3): # é‡è¯•3æ¬¡
        try:
            response = requests.post('https://api.github.com/graphql', json={'query': query, 'variables': variables}, headers=headers, timeout=30)
            response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œåˆ™å¼•å‘HTTPError
            data = response.json()
            if "errors" in data:
                print(f"GraphQL æŸ¥è¯¢å‡ºé”™: {data['errors']}")
                # æ ¹æ®é”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
                if any("RATE_LIMITED" in e.get('type', '') for e in data['errors']):
                    print("è§¦å‘é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…60ç§’åé‡è¯•...")
                    time.sleep(60)
                    continue
            return data
        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/3): {e}")
            time.sleep(5) # ç­‰å¾…5ç§’åé‡è¯•
    raise Exception("å¤šæ¬¡å°è¯•å GraphQL æŸ¥è¯¢ä»ç„¶å¤±è´¥ã€‚")

def fetch_all_listed_repos():
    """é€šè¿‡ç½‘é¡µæŠ“å–è·å–æ‰€æœ‰å…¬å¼€ List ä¸­çš„æ‰€æœ‰é¡¹ç›®"""
    print("æ­£åœ¨é€šè¿‡ç½‘é¡µæŠ“å–è·å–æ‰€æœ‰å…¬å¼€ List...")
    listed_repos = set()
    
    # 1. è®¿é—®ç”¨æˆ· Star é¡µé¢ï¼Œæ‰¾åˆ°æ‰€æœ‰ List çš„é“¾æ¥
    stars_url = f"https://github.com/{USERNAME}?tab=stars"
    print(f"æ­£åœ¨è®¿é—®: {stars_url}")
    try:
        response = requests.get(stars_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾åŒ…å«æ‰€æœ‰ List é“¾æ¥çš„å®¹å™¨
        lists_container = soup.find('div', id='profile-lists-container')
        
        if not lists_container:
            print("è­¦å‘Š: åœ¨ Star é¡µé¢ä¸Šæ²¡æœ‰æ‰¾åˆ° 'profile-lists-container'ã€‚å¯èƒ½é¡µé¢ç»“æ„å·²æ›´æ”¹ã€‚")
            return listed_repos

        # ä»å®¹å™¨ä¸­æ‰¾åˆ°æ‰€æœ‰æŒ‡å‘ List é¡µé¢çš„é“¾æ¥
        list_links = lists_container.find_all('a', href=lambda href: href and href.startswith(f'/stars/{USERNAME}/lists/'))
        
        if not list_links:
            print("è­¦å‘Š: åœ¨ Star é¡µé¢ä¸Šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…¬å¼€çš„ Listã€‚")
            return listed_repos

        list_urls = sorted(list(set(["https://github.com" + a['href'] for a in list_links])))
        print(f"å…±æ‰¾åˆ° {len(list_urls)} ä¸ªå…¬å¼€ Listã€‚")

        # 2. éå†æ¯ä¸ª List é¡µé¢ï¼ŒæŠ“å–é¡¹ç›®
        for i, list_url in enumerate(list_urls):
            print(f"\næ­£åœ¨å¤„ç† List é¡µé¢ '{list_url.split('/')[-1]}' ({i+1}/{len(list_urls)})...")
            try:
                list_response = requests.get(list_url, timeout=30)
                list_response.raise_for_status()
                list_soup = BeautifulSoup(list_response.text, 'html.parser')

                # ä¿®æ­£é€‰æ‹©å™¨ä»¥åŒ¹é…åˆ—è¡¨é¡µé¢çš„HTMLç»“æ„
                repo_tags = list_soup.select('div.col-12 h3 a')
                
                if not repo_tags:
                    print("è­¦å‘Š: åœ¨æ­¤ List é¡µé¢ä¸Šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¡¹ç›®ã€‚")
                    continue

                for tag in repo_tags:
                    repo_name = tag.get('href')
                    if repo_name and repo_name.startswith('/'):
                        repo_name = repo_name[1:] # ç§»é™¤å¼€å¤´çš„ '/'
                        if len(repo_name.split('/')) == 2: # ç¡®ä¿æ˜¯ 'owner/repo' æ ¼å¼
                            listed_repos.add(repo_name)
                            print(f".", end="", flush=True)

            except requests.exceptions.RequestException as e:
                print(f"æŠ“å– List é¡µé¢ {list_url} å¤±è´¥: {e}")

    except requests.exceptions.RequestException as e:
        print(f"è®¿é—® Star ä¸»é¡µé¢å¤±è´¥: {e}")
        raise # å¦‚æœä¸»é¡µéƒ½è®¿é—®ä¸äº†ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸

    print(f"\nå·²ä»æ‰€æœ‰ List ä¸­è®°å½• {len(listed_repos)} ä¸ªç‹¬ç«‹çš„é¡¹ç›®ã€‚")
    return listed_repos

# --- æŸ¥è¯¢è¯­å¥ ---

# æŸ¥è¯¢æ‰€æœ‰ Stars (åŒ…å«åˆ†é¡µå‚æ•°)
stars_query = """
query($user: String!, $cursor: String) {
  user(login: $user) {
    starredRepositories(first: 100, after: $cursor, orderBy: {field: STARRED_AT, direction: DESC}) {
      pageInfo {
        hasNextPage
        endCursor
      }
      nodes {
        nameWithOwner
        url
        description
      }
    }
  }
}
"""


def fetch_all_stars():
    """è·å–ç”¨æˆ·æ‰€æœ‰ Star çš„é¡¹ç›®"""
    print("\næ­£åœ¨è·å–æ‰€æœ‰ Star é¡¹ç›® (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    all_stars = []
    has_next = True
    cursor = None
    page_count = 0

    while has_next:
        page_count += 1
        print(f".", end="", flush=True)
        if page_count % 50 == 0: # æ¯50é¡µæ¢è¡Œ
             print("")

        data = get_graphql_data(stars_query, {"user": USERNAME, "cursor": cursor})
        stars_data = data.get('data', {}).get('user', {}).get('starredRepositories', {})

        if not stars_data:
            print("è­¦å‘Š: æ— æ³•è·å– Star æ•°æ®ã€‚")
            break

        all_stars.extend(stars_data.get('nodes', []))
        
        has_next = stars_data.get('pageInfo', {}).get('hasNextPage', False)
        cursor = stars_data.get('pageInfo', {}).get('endCursor')

    print(f"\nå…±è·å–åˆ° {len(all_stars)} ä¸ª Star é¡¹ç›®ã€‚")
    return all_stars


def generate_markdown(uncategorized):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    filename = "uncategorized_stars.md"
    try:
        # ä½¿ç”¨ 'date -u' è·å– UTC æ—¶é—´ï¼Œæ›´ç¬¦åˆå›½é™…æ ‡å‡†
        date_str = os.popen('date -u').read().strip()
    except:
        date_str = "æ— æ³•è·å–æ—¶é—´"

    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# æœªåˆ†ç±» Stars æ¸…å•\n\n")
        f.write(f"> ç”Ÿæˆäº UTC æ—¶é—´: {date_str} | æ€»è®¡: **{len(uncategorized)}** ä¸ªæœªåˆ†ç±»é¡¹ç›®\n\n")
        
        if not uncategorized:
            f.write("ğŸ‰ æ­å–œï¼æ‰€æœ‰ Star çš„é¡¹ç›®éƒ½å·²åˆ†ç±»ã€‚\n")
        else:
            f.write("| é¡¹ç›® (Repository) | æè¿° (Description) |\n")
            f.write("| --- | --- |\n")
            for repo in sorted(uncategorized, key=lambda x: x['nameWithOwner'].lower()): # æŒ‰å­—æ¯æ’åº
                # é˜²å¾¡æ€§åœ°è·å–å­—æ®µ
                name = repo.get('nameWithOwner', 'æœªçŸ¥é¡¹ç›®')
                url = repo.get('url', '#')
                desc = repo.get('description', 'æš‚æ— æè¿°')
                
                # æ¸…ç†æè¿°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                if desc:
                    desc = desc.replace("\n", " ").replace("\r", " ").replace("|", "/")
                    if len(desc) > 80: desc = desc[:77] + "..."
                
                f.write(f"| [{name}]({url}) | {desc} |\n")
    
    print(f"\næŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")

def main():
    """ä¸»ç¨‹åº"""
    try:
        listed_repos = fetch_all_listed_repos()
        all_stars = fetch_all_stars()
        
        uncategorized = [repo for repo in all_stars if repo.get('nameWithOwner') and repo['nameWithOwner'] not in listed_repos]

        generate_markdown(uncategorized)
        print("\nâœ… ä»»åŠ¡æˆåŠŸå®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        # åœ¨ GitHub Actions ç¯å¢ƒä¸­ï¼Œå°†é”™è¯¯å†™å…¥ GITHUB_STEP_SUMMARY
        if "GITHUB_STEP_SUMMARY" in os.environ:
            with open(os.environ["GITHUB_STEP_SUMMARY"], "a") as f:
                f.write("## è„šæœ¬è¿è¡Œå¤±è´¥\n\n")
                f.write(f"é”™è¯¯è¯¦æƒ…: `{e}`\n")
        sys.exit(1)

if __name__ == "__main__":
    main()
